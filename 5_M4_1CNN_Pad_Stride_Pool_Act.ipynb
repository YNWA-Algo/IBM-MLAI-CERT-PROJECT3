{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YNWA-Algo/IBM-MLAI-CERT-PROJECT3/blob/main/5_M4_1CNN_Pad_Stride_Pool_Act.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39c55cff-8327-4005-8562-18b09ea1eb83"
      },
      "source": [
        "<center>\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f36be485-a734-45e7-96d5-778599860f21"
      },
      "source": [
        "# **Padding, Stride, Pooling, Activation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b5f54a6-821a-40f8-a8bd-ad0c3838d6f1"
      },
      "source": [
        "Estimated time needed: 50 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b7be679-5211-466b-a6df-d6fba47d3b48"
      },
      "source": [
        "In the image convolution lab, you saw how image convolutions could be used to detect features in an image, such as edges, corners, blobs, etc. However, it could be challenging to develop an intuition of how the shape of the kernel as well as other related configuratiton parameters of a Convolution layer would affect the shape of the output.\n",
        "\n",
        "In CNN, having a concrete understanding of the size of the output of each layer is necessary. Hence, in this lab, we will dive into some of the important factors to consider when working with CNNs, namely, Padding, Stride, Pooling, and Activation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9a5eda4-7503-4509-8c46-61eceaa1c82e"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "<ol>\n",
        "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
        "    <li>\n",
        "        <a href=\"#Setup\">Setup</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
        "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
        "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "    <li>\n",
        "        <a href=\"#Padding and Stride\">Padding and Stride</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Background\">Background</a></li>\n",
        "            <li><a href=\"#How-does-Padding-work?\">How does Padding work?</a></li>\n",
        "            <li><a href=\"#Using Padding in Tensorflow.keras\">Using Padding in Tensorflow.keras</a></li>\n",
        "            <li><a href=\"#How-does-Stride-work?\">How does Stride work?</a>\n",
        "            <li><a href=\"#Example 1: Image Processing - Edge Detection\">Example 1: Image Processing - Edge Detection</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "    <li><a href=\"Activation\">Activation</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Example 2: Feature Detection with Kernel and Activation\">Example 2: Feature Detection with Kernel and Activation</a></li>\n",
        "        </ol>           \n",
        "    </li>\n",
        "    <li><a href=\"Pooling\">Pooling</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#What is Pooling?\">What is Pooling?</a></li>\n",
        "            <li><a href=\"#Example 3: Max or Average?\">Example 3: Max or Average?</a></li>      \n",
        "        </ol>     \n",
        "    </li>      \n",
        "    <li><a href=\"#Exercises\">Exercises</a>\n",
        "        <ol>\n",
        "            <li><a href=\"#Exercise 1 - Display some images\">Exercise 1 - Display some images</a></li>\n",
        "            <li><a href=\"#Exercise 2 - Set up a Convolution layer\">Exercise 2 - Set up a Convolution layer</a></li>\n",
        "            <li><a href=\"#Exercise 3 - Set up a Max Pooling layer\">Exercise 3 - Set up a Max Pooling layer</a></li>\n",
        "            <li><a href=\"#Exercise 4 - Create a deeper CNN with blocks\">Exercise 4 - Create a deeper CNN with blocks</a></li>\n",
        "            <li><a href=\"#Optional section\">Optional section</a></li>\n",
        "        </ol>\n",
        "    </li>\n",
        "</ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f29222-b62c-482f-8ff7-f0c924a263d8"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "After completing this lab you will be able to:\n",
        "\n",
        " - Understand the use of padding and stride in CNN\n",
        " - Calculate the size of the output of a layer\n",
        " - Understand the necessity of activations\n",
        " - Describe the difference between Max pooling and Average Pooling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6f9f92-f463-4f11-94c9-42f2366c83bf"
      },
      "source": [
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bc21023-2092-4e80-b06f-e3e68970466a"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c33d1b2-9a38-4dd8-89b0-092a0afb9a9a"
      },
      "source": [
        "For this lab, we will be using the following libraries:\n",
        "\n",
        "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
        "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
        "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
        "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
        "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a62f4d7-90fc-426a-9a75-4d607edd5bf3"
      },
      "source": [
        "### Installing Required Libraries\n",
        "\n",
        "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6297e7bf-1264-45c9-b8ab-314a40ea71e1",
        "outputId": "03a4ecdd-6921-4d85-b85b-b87806907c89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'opencv-python' candidate (version 4.5.5.62 at https://files.pythonhosted.org/packages/9d/98/36bfcbff30da27dd6922ed73ca7802c37d87f77daf4c569da3dcb87b4296/opencv_python-4.5.5.62-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from https://pypi.org/simple/opencv-python/) (requires-python:>=3.6))\n",
            "Reason for being yanked: deprecated, use 4.5.5.64\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.22.3 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.22.3 which is incompatible.\n",
            "arviz 0.20.0 requires numpy>=1.23.0, but you have numpy 1.22.3 which is incompatible.\n",
            "astropy 6.1.7 requires numpy>=1.23, but you have numpy 1.22.3 which is incompatible.\n",
            "bigframes 1.29.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.1 which is incompatible.\n",
            "bigframes 1.29.0 requires numpy>=1.24.0, but you have numpy 1.22.3 which is incompatible.\n",
            "chex 0.1.88 requires numpy>=1.24.1, but you have numpy 1.22.3 which is incompatible.\n",
            "contourpy 1.3.1 requires numpy>=1.23, but you have numpy 1.22.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.3 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-aiplatform 1.74.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.27.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-bigtable 2.27.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-datastore 2.20.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-firestore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-functions 1.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-iam 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-language 2.16.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-pubsub 2.27.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-resource-manager 1.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "google-cloud-translate 3.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "googleapis-common-protos 1.66.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.1 requires protobuf!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n",
            "grpcio-status 1.62.3 requires protobuf>=4.21.6, but you have protobuf 3.19.6 which is incompatible.\n",
            "ibis-framework 9.2.0 requires numpy<3,>=1.23.2, but you have numpy 1.22.3 which is incompatible.\n",
            "jax 0.4.33 requires numpy>=1.24, but you have numpy 1.22.3 which is incompatible.\n",
            "jaxlib 0.4.33 requires numpy>=1.24, but you have numpy 1.22.3 which is incompatible.\n",
            "langchain 0.3.12 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 1.22.3 which is incompatible.\n",
            "mizani 0.13.1 requires numpy>=1.23.5, but you have numpy 1.22.3 which is incompatible.\n",
            "numexpr 2.10.2 requires numpy>=1.23.0, but you have numpy 1.22.3 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.3 which is incompatible.\n",
            "pandas 2.2.2 requires numpy>=1.22.4; python_version < \"3.11\", but you have numpy 1.22.3 which is incompatible.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "pandas-stubs 2.2.2.240909 requires numpy>=1.23.5, but you have numpy 1.22.3 which is incompatible.\n",
            "plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.5.1 which is incompatible.\n",
            "plotnine 0.14.4 requires numpy>=1.23.5, but you have numpy 1.22.3 which is incompatible.\n",
            "pylibraft-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.3 which is incompatible.\n",
            "pymc 5.19.1 requires numpy>=1.25.0, but you have numpy 1.22.3 which is incompatible.\n",
            "rmm-cu12 24.10.0 requires numpy<3.0a0,>=1.23, but you have numpy 1.22.3 which is incompatible.\n",
            "scikit-image 0.25.0 requires numpy>=1.24, but you have numpy 1.22.3 which is incompatible.\n",
            "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 1.22.3 which is incompatible.\n",
            "tensorflow-datasets 4.9.7 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.9.0 which is incompatible.\n",
            "xarray 2024.11.0 requires numpy>=1.24, but you have numpy 1.22.3 which is incompatible.\n",
            "xarray-einstats 0.8.0 requires numpy>=1.23, but you have numpy 1.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
        "\n",
        "!pip install -q numpy==1.22.3 matplotlib==3.5.1 tensorflow==2.9.0 opencv-python==4.5.5.62\n",
        "\n",
        "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install --user\"\n",
        "\n",
        "# RESTART YOUR KERNEL AFTERWARD AS WELL"
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "809ac8bf-8698-4a8a-98b2-53b313252ed0"
      },
      "source": [
        "The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1a089a6-31d5-461c-8282-512da4adfca6"
      },
      "outputs": [],
      "source": [
        "!pip install -q tqdm"
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a541a91-94b7-498a-a0bd-80566609ca40",
        "outputId": "3fa6ca53-ae63-44d0-980c-78a0e5654ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting keras>=3.5.0 (from tensorflow)\n",
            "  Downloading keras-3.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow)\n",
            "  Downloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
            "Downloading keras-3.7.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flatbuffers, tensorboard-data-server, protobuf, numpy, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorboard-data-server\n",
            "    Found existing installation: tensorboard-data-server 0.6.1\n",
            "    Uninstalling tensorboard-data-server-0.6.1:\n",
            "      Successfully uninstalled tensorboard-data-server-0.6.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.3\n",
            "    Uninstalling numpy-1.22.3:\n",
            "      Successfully uninstalled numpy-1.22.3\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.0\n",
            "    Uninstalling tensorflow-2.9.0:\n",
            "      Successfully uninstalled tensorflow-2.9.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.29.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.1 which is incompatible.\n",
            "cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.2 which is incompatible.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\n",
            "langchain 0.3.12 requires numpy<2,>=1.22.4; python_version < \"3.12\", but you have numpy 2.0.2 which is incompatible.\n",
            "pandas-gbq 0.25.0 requires google-auth-oauthlib>=0.7.0, but you have google-auth-oauthlib 0.4.6 which is incompatible.\n",
            "plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.5.1 which is incompatible.\n",
            "pytensor 2.26.4 requires numpy<2,>=1.17.0, but you have numpy 2.0.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\n",
            "thinc 8.2.5 requires numpy<2.0.0,>=1.19.0; python_version >= \"3.9\", but you have numpy 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-24.3.25 keras-3.7.0 numpy-2.0.2 protobuf-5.29.2 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2cb91d1-aa76-441a-b2eb-0f2726303ca8"
      },
      "source": [
        "### Importing Required Libraries\n",
        "\n",
        "_We recommend you import all required libraries in one place (here):_\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "d7014726-7657-4daf-aef9-69a1d403620e",
        "outputId": "8664ac55-9f65-4296-bde8-296e47dced1f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cb1bdd328d14>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccumulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpalettes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrelational\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F401,F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_compat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgroupby_apply_include_groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_statistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimateAggregator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWeightedAggregator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maxisgrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFacetGrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_facet_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_docstrings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDocstringComponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_core_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/_statistics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_kde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0m_no_scipy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    604\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_mstats_basic\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_discrete_distns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_continuous_distns\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/stats/_discrete_distns.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetaln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgammaln\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgamln\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_lazywhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrng_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterp1d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfloor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog1p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mshould\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_interpolate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fitpack_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_interpolate.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fitpack_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdfitpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_polyint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_Interpolator1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# These are in the API for fitpack even if not used in fitpack.py itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_fitpack_impl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbisplrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbisplev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdblint\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_fitpack_impl\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_impl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bsplines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBSpline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack_impl.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m _parcur_cache = {'t': array([], float), 'wrk': array([], float),\n\u001b[0;32m--> 103\u001b[0;31m                  \u001b[0;34m'iwrk'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfitpack_int\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'u'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                  'ub': 0, 'ue': 1}\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import accumulate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_digits, load_wine\n",
        "\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import PIL\n",
        "from PIL import Image, ImageOps\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, datasets\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "\n",
        "sns.set_context('notebook')\n",
        "sns.set_style('white')"
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61531c9a-9fc6-44e6-89c6-2fd0b5cf620e"
      },
      "source": [
        "### Defining Helper Functions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a020e87-283e-4334-8892-5d7784329bf3"
      },
      "source": [
        "The Sobel operator for edge detection **v_grad** and **h_grad** will be defined later.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b48b425-9fdd-493b-beeb-97bf561bfa9d"
      },
      "outputs": [],
      "source": [
        "# This function will allow us to easily plot data taking in x values, y values, and a title\n",
        "def sobel(img, strides, padding, activation=None):\n",
        "\n",
        "    input_layer = layers.Input(shape=(img_height, img_width, 1))\n",
        "\n",
        "    v_conv = layers.Conv2D(filters=1,\n",
        "                       kernel_size=3,\n",
        "                       kernel_initializer=v_grad,\n",
        "                       strides=strides,\n",
        "                       padding=padding,\n",
        "                       activation=None)\n",
        "    h_conv = layers.Conv2D(filters=1,\n",
        "                   kernel_size=3,\n",
        "                   kernel_initializer=h_grad,\n",
        "                   strides=strides,\n",
        "                   padding=padding,\n",
        "                   activation=None)\n",
        "\n",
        "    v_model = keras.Sequential([input_layer, v_conv])\n",
        "    h_model = keras.Sequential([input_layer, h_conv])\n",
        "\n",
        "    out_d = h_model.layers[0].output_shape[1:]\n",
        "    Gx = h_model.predict(img).reshape(out_d)\n",
        "    Gy = v_model.predict(img).reshape(out_d)\n",
        "    G = np.sqrt(np.add(np.multiply(Gx, Gx), np.multiply(Gy, Gy)))\n",
        "\n",
        "    return G"
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14a14447-ae93-4418-8422-9ef96de6672b"
      },
      "source": [
        "## Padding and Stride\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e37638-d15d-4955-98f5-d86a0849a050"
      },
      "source": [
        "### Background\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4c71176-c83b-4630-a794-d510f6697caf"
      },
      "source": [
        "In our previous lab where we introduced image convolutions, we mentioned that we tend to lose the pixels on the border of our input image because there are not enough pixels surrounding them to support the kernel matrix multiplications. This could be detrimental to our CNN model performance since we will increasingly lose information as we apply many successive convolution layers.\n",
        "\n",
        "To avoid losing important information carried by border pixels and control the size of our convolution output at the same time, one solution would be to use **Padding** and **Stride** when shifting a kernel on the input image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec69b7e6-832e-40a1-ab0e-9c6137acc3f7"
      },
      "source": [
        "### How does Padding work?\n",
        "\n",
        "Padding essentially extends the perimeter of our image by adding extra pixels, commonly **zero-valued**, on the outer frame of the image. This is done so that the border pixels could also be at or near the center of the receptive field when the kernel window slides through, and the information is retained as useful features for the next step.\n",
        "\n",
        "The following figure illustrates the case where we apply a $3\\times3$ kernel on a $3\\times3$ image array with 2 rows of padding (1 on top, 1 on bottom) and 2 columns of padding (1 on left, 1 on right):\n",
        "\n",
        "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/pad1_blue_.jpg\" width=\"50%\"></center>\n",
        "\n",
        "As the kernel window slides through the padded image, we will obtain an output image with the same size as the input, which is $3\\times3$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "334f51cb-e7b5-4ffa-8233-764987c89bcf"
      },
      "source": [
        "In general, if we add a total of $p_r$ rows of padding and $p_c$ columns of padding to a $m\\times n$ input, the output shape after applying a $k\\times k$ kernel will be:\n",
        "\n",
        "$$(m+p_r-k+1)\\times (n+p_c-k+1)$$\n",
        "\n",
        "This formula implies that if we want the size of input and output to be the same, we need to make sure that $p_r=k-1=p_c$. As we would pad on the left and right, top and bottom of an image, $p_r$ and $p_c$ are typically even numbers so that all the previous layer pixels would be symmetrical around the outout pixel. This is why we tend to choose odd kernel sizes, such as $3\\times3$, $5\\times5$, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5e7a70f-cd5d-4ed6-ae4f-8f80ab6bb14b"
      },
      "source": [
        "### Use Padding in Tensorflow.Keras\n",
        "\n",
        "In Keras, the operation of padding is specified via the `padding` argument in `keras.layers.Conv2D`. The default is `padding = 'valid'`, which means no padding. Setting `padding = 'same'` will calculate the required padding size such that the input and output will have the same shape and also add the padding, given an input and a kernel size.\n",
        "\n",
        "The example below adds padding to a convolutional layer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "622b774b-83f2-44d9-9d71-d05d480b0b82",
        "outputId": "cb5367c1-7238-4999-fcc4-411077c719c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │              \u001b[38;5;34m10\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10\u001b[0m (40.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> (40.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10\u001b[0m (40.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> (40.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 1,\n",
        "                 kernel_size = (3,3),\n",
        "                 padding = 'same',\n",
        "                 input_shape = (10, 10, 1)))\n",
        "model.summary()"
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23e95e57-ed73-4481-b407-72583a7fa83f"
      },
      "source": [
        "So we built a simple CNN model with one convolution layer, where we specified an input shape of $10\\times10$ with one channel and the kernel applied is $3\\times3$. Via padding, the output shape is also $10\\times10$!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab6b9f1c-d30b-4d8e-9d60-8969565fbd7f"
      },
      "source": [
        "### How does Stride work?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92d56416-d77a-4c9a-afea-f0af0773d522"
      },
      "source": [
        "In the previous example, we defaulted to shifting the kernel window one pixel at a time across the image left to right, top to bottom. However, sometimes for computational efficiency or because we simply wish to downsample, we could choose to move the kernel window more than one pixel at a time.\n",
        "\n",
        "The number of rows and columns traversed per slide is referred to as the **Stride**. In Keras, we could specify the stride in `keras.layers.Conv2D` via the `strides` argument, as a tuple of 2 integers. For example, `strides = (2,3)` means we shift the kernel 2 pixels right for each horizontal movement and 3 pixels down for each vertical movement.\n",
        "\n",
        "**Combining the concept of Padding and Stride**, if we add a total of $p_r$ rows of padding and $p_c$ columns of padding to a $m\\times n$ input, the output shape after applying a $k\\times k$ kernel with $(s_r, s_c)$ stride will be:\n",
        "\n",
        "$$[(m+p_r-k) / s_r+1]\\times [(n+p_c-k) / s_c+1]$$\n",
        "\n",
        "\n",
        "To illustrate, the full convolution process using one $3\\times3$ kernel on a $3\\times3$ input image with padding in place and `strides = (2,2)` looks like:\n",
        "\n",
        "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/paddingstride.gif\" width=\"85%\"></center>\n",
        "\n",
        "We can code this in Keras:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "8cb48276-5381-4357-b40f-7bfa63ec468e",
        "outputId": "b5105ac0-c9d9-4bd5-b454-caf8dcdc4f2b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1\u001b[0m)             │              \u001b[38;5;34m10\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10\u001b[0m (40.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> (40.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10\u001b[0m (40.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> (40.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 1,\n",
        "                 kernel_size = (3, 3),\n",
        "                 strides = (2, 2),\n",
        "                 padding = \"same\",\n",
        "                 input_shape = (3, 3, 1)))\n",
        "model.summary()"
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbf9522a-74f8-4cb1-a4d5-187ddd0ac1c6"
      },
      "source": [
        "So the output shape calculated by Keras is indeed $2\\times2\\times1$, where the one indicates the output channel (because we only applied one kernel).\n",
        "\n",
        "We can code the input image array and the $3\\times3$ kernel, and then use a simple CNN with one **Conv2D** layer to predict the output values.\n",
        "\n",
        "**NOTE:** Before feeding the input into the CNN, we need to reshape it so that the first **1** represents the batch size and the last **1** represents the number of channels the input contains.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "fbcc8ad7-bcbc-4873-9fec-61a62537f470",
        "outputId": "f3ceaa0d-e939-4b24-efb6-7f40df887ef4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Value returned by __array__ is not a NumPy array",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-ba58cbb72fa4>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0moutput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    706\u001b[0m                     \u001b[0;34mf\"shape {value.shape}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 )\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mvariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/common/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36m_convert_to_tensor\u001b[0;34m(self, value, dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/core.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Value returned by __array__ is not a NumPy array"
          ]
        }
      ],
      "source": [
        "input_ = np.array([[1, 1, 3],\n",
        "              [2, 1, 2],\n",
        "              [3, 1, 4]]).reshape(1, 3, 3, 1)\n",
        "\n",
        "kernel = np.array([[1, 0, -1],\n",
        "                   [1, 0, -1],\n",
        "                   [1, 0, -1]]).reshape(3, 3, 1, 1) # reshaping the kernel is important\n",
        "\n",
        "b = np.array([0.0])\n",
        "\n",
        "model.set_weights([kernel, b])\n",
        "output_ = model.predict(input_)\n",
        "\n",
        "for r in range(output_.shape[1]):\n",
        "    print([output_[0,r,c,0] for c in range(output_.shape[2])])"
      ],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a7a3124-1d0f-4f89-a4e4-0f1d6cbba900"
      },
      "source": [
        "**With the same input and kernel, the output matrix of our CNN is the same as the one shown in the animated gif.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5c94b9b-112e-4c57-99e8-11858967edc2"
      },
      "source": [
        "### Example 1: Image Processing - Edge Detection\n",
        "\n",
        "In the last image convolution lab, you learnt about the Sobel Operator which uses two kernels to convolve with an image to perform edge detection. In this example, we will use the Sobel Operator to detect edges in the image of the [Leaning Tower of Pisa](https://pxhere.com/en/photo/1027167) with padding and stride.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01b6df6c-6fd8-4f31-8c7a-61d18e082f61"
      },
      "source": [
        "Let's first define the two kernels used for the [Sobel Operator](https://en.wikipedia.org/wiki/Sobel_operatorhttps://en.wikipedia.org/wiki/Sobel_operator):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9979892e-70cf-4ffd-b3e5-b591cf6829b6"
      },
      "outputs": [],
      "source": [
        "# Lets define our custom kernels for the horizontal and vertical gradients\n",
        "def v_grad(shape, dtype=None):\n",
        "    # Here we use a single numpy array to define our x gradient kernel\n",
        "    grad = np.array([\n",
        "        [1, 0, -1],\n",
        "        [2, 0, -2],\n",
        "        [1, 0, -1]\n",
        "    ]).reshape((3, 3, 1, 1))\n",
        "    # this line is quite important, we are saying we want one 3x3 kernel each for one channel of pixels (grayscale)\n",
        "\n",
        "    # We check to make sure the shape of our kernel is the correct shape\n",
        "    # according to the initialization of the Convolutional layer below\n",
        "    assert grad.shape == shape\n",
        "    return keras.backend.variable(grad, dtype='float32')\n",
        "\n",
        "def h_grad(shape, dtype=None):\n",
        "    grad = np.array([\n",
        "        [1, 2, 1],\n",
        "        [0, 0, 0],\n",
        "        [-1, -2, -1]\n",
        "        ]).reshape((3, 3, 1, 1))\n",
        "\n",
        "    assert grad.shape == shape\n",
        "    return keras.backend.variable(grad, dtype='float32')"
      ],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94cb3e5e-d204-492e-9b23-4632f785fa28"
      },
      "source": [
        "Download the image and display it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6950a60-7578-40a3-9a4c-8ec88e62287f",
        "outputId": "4111e030-1942-4865-d6b4-560944e747ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-19 15:10:01--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/pisa.jpg\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 198.23.119.245\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|198.23.119.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 762548 (745K) [image/jpeg]\n",
            "Saving to: ‘pisa.jpg’\n",
            "\n",
            "pisa.jpg            100%[===================>] 744.68K  3.12MB/s    in 0.2s    \n",
            "\n",
            "2024-12-19 15:10:02 (3.12 MB/s) - ‘pisa.jpg’ saved [762548/762548]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/pisa.jpg\""
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "b0983919-71ce-41cb-9f1c-76e4d34db3f5",
        "outputId": "a8d4e16e-b697-4465-a428-777771690367"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'PIL' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ee83bf9d93e1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m350\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_height\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pisa.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PIL' is not defined"
          ]
        }
      ],
      "source": [
        "img_width = 350\n",
        "img_height = 500\n",
        "img = PIL.Image.open(\"pisa.jpg\").resize((img_width, img_height))\n",
        "img"
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d48cabe7-dc5b-4078-ab6a-ed45eacacfb7"
      },
      "source": [
        "Now we set `padding = 'same'` and try different values of `strides` for convolving the two kernels of the Sobel operator over the image. We will use the pre-defined function **sobel**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b52889a9-456b-41f4-8d46-b36784d9e0ec"
      },
      "outputs": [],
      "source": [
        "input_img = ImageOps.grayscale(img)\n",
        "input_img = np.array(input_img).reshape((1, img_height, img_width, 1))\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(9, 10), constrained_layout=True)\n",
        "fig.set_constrained_layout_pads(w_pad=0.01, h_pad=0.02, hspace=0, wspace=0.1)\n",
        "\n",
        "for i, ax in enumerate(axs.flat):\n",
        "    output = sobel(img = input_img,\n",
        "                   padding='same',\n",
        "                   strides=i+1).astype('int').clip(0,255)\n",
        "\n",
        "    ax.imshow(output, cmap='gray')\n",
        "    ax.set_title(f\"Strides: {i+1}, Shape: {output.shape}\", fontsize=13)\n",
        "    ax.axis('off')"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4148ae52-2625-4030-a822-5d20ea83bbea"
      },
      "source": [
        "As we increase the value of `strides`, the output shape becomes proportionally smaller (as shown in the image titles) but edges detected also become blurry. Therefore, whether to use stride and what value to use may be highly dependent on the specific problem at hand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42338d74-943a-4f47-827f-e2a14a41ecd6"
      },
      "source": [
        "## Activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027947c8-6380-4aad-afbf-a7681b480f5d"
      },
      "source": [
        "Recall in our previous coded example or in the animated gif, the output matrix contains negative values such as -2. However, since CNN is typically implemented on images which should consist of pixels values ranging from 0 to 255, we need to make our CNN valid by adding what's called an activation after the convolution operation.\n",
        "\n",
        "In addition to the need of restricting certain limits for the values in our matrices, activations are also necessary for adding **non-linearity** into our network so that the network can learn complex patterns in the data.\n",
        "\n",
        "In `tensorflow.keras.layers.Conv2D`, there is an argument called `activation` where we can specify the type of activation we want for the current convolution layer added. Currently, there are many types of activations existing, such as Sigmoid, Tanh, ReLU, ..., etc. For example, the famous **Sigmoid** function is defined as follows:\n",
        "\n",
        "$$ f(x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^{-x}}  $$\n",
        "\n",
        "We can code the sigmoid function using numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "860c592e-a08e-4dca-9c12-fecd93a951bf"
      },
      "outputs": [],
      "source": [
        "def sigmoid(X):\n",
        "\n",
        "    return 1/(1 + np.exp(-X))\n",
        "\n",
        "X = np.linspace(-10, 10, 100)\n",
        "sigmoid_X = sigmoid(X)\n",
        "plt.plot(X, sigmoid_X)\n",
        "plt.axhline(y=0.5, color='r', linestyle='-')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Sigmoid(x)\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c802bafc-c028-478c-953f-0247351416be"
      },
      "source": [
        "The sigmoid function is also called a **squashing function**. The input to the sigmoid activation is usually a weighted sum of the outputs of the previous layer. If the input is either a very large negative number or a very large positive number, the output of the activation is always between 0 and 1.\n",
        "\n",
        "\n",
        "Other than the sigmoid activation, you can also read on the different kinds of activations [here](https://en.wikipedia.org/wiki/Activation_function). For this lab we will use the **ReLU** activation.\n",
        "\n",
        "**ReLU** or **Rectified Linear Unit**, is a widely used activation function, especially with Convolutional Neural networks. It is defined as:\n",
        "\n",
        "$$f(x) = max(0, x)$$\n",
        "\n",
        "where x is the matrix obtained after convolution. By using ReLU, we can filter out all the negative values as they will become zero's and just keep the positive values.\n",
        "\n",
        "We can code the relu activation using numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0df6ba0e-f390-42aa-8436-158e849d835b"
      },
      "outputs": [],
      "source": [
        "def relu(X):\n",
        "    return np.maximum(0, X)\n",
        "\n",
        "X = np.linspace(-10, 10, 100)\n",
        "relu_X = relu(X)\n",
        "plt.plot(X, relu_X)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Relu(x)\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d49175dd-cf98-4ad4-8067-117e33b6f8a7"
      },
      "source": [
        "### Example 2: Feature Detection with Kernel and Activation\n",
        "\n",
        "In this example, instead of using the Sobel operator for edge detection, we will convolve a simple kernel over all three channels of a RGB image for the same purpose. This means our final result of edge detection will no longer be a boring, grayscale image, but a colorful and exciting one!\n",
        "\n",
        "Specifically, we will use the following kernel:\n",
        "\n",
        "$$\n",
        "\t\\begin{bmatrix}\n",
        "\t-1 & -1 & -1 \\\\\\\\\\\\\n",
        "\t-1 & 8 & -1\\\\\\\\\n",
        "\t-1 & -1 & -1\\\\\\\\\n",
        "\t\\end{bmatrix}\n",
        "\t\\quad\n",
        "$$\n",
        "\n",
        "Note that the kernel we will be using contains negative values, therefore we should specify an activation function (namely **ReLu**) in our Conv2D layer to filter out the negative values.\n",
        "\n",
        "Let's download the image first!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28408426-fe40-4537-ba6f-37aa418e48c8"
      },
      "outputs": [],
      "source": [
        "!wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/lambor.jpeg\""
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6f8d672-6712-4f4a-8e28-14e363607bf5"
      },
      "outputs": [],
      "source": [
        "img_width = 500\n",
        "img_height = 280\n",
        "\n",
        "img = Image.open(\"lambor.jpeg\").resize((img_width, img_height))\n",
        "img"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52dfa4a0-6afc-45b9-b866-feb4cba960b6"
      },
      "source": [
        "As we will be convolving the kernel over each channel, we need to split the RGB image into three channels or three grayscale images.\n",
        "\n",
        "Let's display the three channels:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52f7f4d3-f191-4482-a9fa-c9c6d610ff7e"
      },
      "outputs": [],
      "source": [
        "arr = np.array(img)\n",
        "red_c = arr[:,:,0]\n",
        "green_c = arr[:,:,1]\n",
        "blue_c = arr[:,:,2]\n",
        "\n",
        "channels = [red_c, green_c, blue_c]\n",
        "names = [\"Red\", \"Green\", \"Blue\"]\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(channels[i], cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{names[i]} channel\", fontsize=13)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5c30476-9bfd-4245-8f94-c50c128bb17f"
      },
      "source": [
        "You can see that the color of the car is mostly coming from the red and green channel, and very little from the blue channel.\n",
        "\n",
        "Now we build a simple CNN with one Conv2D layer, using the kernel specified above as the weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d4b75a4-6865-474b-805b-722d88915d9d"
      },
      "outputs": [],
      "source": [
        "kernel = np.array([[-1,-1,-1],\n",
        "                   [-1,8,-1],\n",
        "                   [-1,-1,-1]]).reshape(3,3,1,1)\n",
        "b = np.array([0.0])\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Conv2D(input_shape = (img_height, img_width, 1),\n",
        "                 filters=1,\n",
        "                 kernel_size=3,\n",
        "                 padding='same',\n",
        "                 activation='relu'\n",
        "                 ))\n",
        "model.set_weights([kernel, b])\n",
        "model.summary()"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf94a81e-e373-4af5-8f96-d6e8f4fca6e6"
      },
      "source": [
        "Once we initialize our CNN, we can now use it to predict the edge detection result for each of the R, G, B channel's gray image!\n",
        "\n",
        "Don't worry if the predicted results look almost all-black to you. Remember we used the ReLu activation to set all the negative pixel values to 0. In each channel, if the pixel value is 0, the pixel will appear black. Only the important information will be kept.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fae5bf70-5034-4675-b258-667551ffc6f6"
      },
      "outputs": [],
      "source": [
        "acts = []\n",
        "plt.figure(figsize=(18,5))\n",
        "\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "\n",
        "    # loop through each channel\n",
        "    input_ = channels[i].reshape((1, img_height, img_width, 1))\n",
        "    act = model.predict(input_).squeeze(0).squeeze(2).astype('int').clip(0,255)\n",
        "    # store the result in a list called \"acts\"\n",
        "    acts.append(act)\n",
        "\n",
        "    plt.imshow(act, cmap='gray')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(f\"{names[i]} Channel with activation\", fontsize=13)"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d604decb-f7e8-4a2c-afc2-3851922643af"
      },
      "source": [
        "Now it's time to stack the three channels back together and see what our edge detection tool did for us!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ef1d61f-c196-4599-8aca-e49e9578daac"
      },
      "outputs": [],
      "source": [
        "arr_hat = np.dstack((acts[0],acts[1],acts[2]))\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(arr_hat)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16a09c66-60e4-4c51-9354-e53500dc4500"
      },
      "source": [
        "Since we applied the edge detector on the three color channels separately, once we merge the results after the convolutions we obtain colorful edges of the car. Each colored edge that you see in the picture above is contributed by some degree of red edges detected in channel R, green edges detected in channel G, and blue edges in channel B.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a549d62f-534a-4f7a-82d3-37c4abc76707"
      },
      "source": [
        "## Pooling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67c6050-7943-4fbd-aa5a-ce1791336f04"
      },
      "source": [
        "### What is Pooling?\n",
        "\n",
        "A CNN model may include local and/or global pooling layers along with traditional convolutional layers. It is a form of non-linear down-sampling to progressively **reduce the spatial size of the representation**, to **reduce the number of parameters**, **memory footprint and amount of computation in the network**, and hence to also **control overfitting**.\n",
        "\n",
        "There are two common types of pooling in popular use: **Max Pooling** and **Average Pooling**. Max pooling outputs the maximum value of each sub-region in the feature map, while average pooling outputs the average value.\n",
        "\n",
        "It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture. You can read more about Pooling [here](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layer).\n",
        "\n",
        "An illustration of **Max Pooling**:\n",
        "\n",
        "<center><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/images/maxpooling.gif\" width=\"1100\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a288a7a-d470-4105-9e04-4b705945c43f"
      },
      "source": [
        "In tensorflow.keras, a **Pooling layer** downsamples the input along its spatial dimensions (height and width) by taking the maximum or average value over an input window (of size defined by `pool_size`) for each channel of the input.\n",
        "\n",
        "You can add a Pooling layer to a CNN with **MaxPooling2D** or **AveragePooling2D**. Like a **Conv2D** layer, a Pooling layer also has arguments such as `padding` and `strides`, and they work in a same way as in a Conv2D layer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c4cbb3a-3d3a-4f5d-aedc-6662c9039ff6"
      },
      "source": [
        "### Example 3: Max or Average?\n",
        "\n",
        "- Max pooling **retains the most prominent features in the feature map**. It is useful when the background of the image is dark and we are interested in the lighter pixels of the image. For example, in the MNIST dataset, the digits are represented in white color and the background is black, so max pooling is more suitable.\n",
        "\n",
        "- Average pooling **tends to smooth out the image**. Sometimes it cannot extract the important features because it takes everything into account, and gives an average value which may or may not be important. It shows a similar effect irrespective of the background.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a71672cc-419b-41ac-a572-e112e76670a0"
      },
      "source": [
        "Run the next cell to download the MNIST datset, which we will use to illustrate the difference between the output of Max Pooling and Average Pooling.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "80eb3978-b37a-4243-b064-57c9ac84a4ed",
        "outputId": "029e8bc4-1eb7-4278-d736-a5fae39372a9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b8d6b91ac1fe>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MNIST downloaded!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train set shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print(\"MNIST downloaded!\")\n",
        "print(\"Train set shape:\", X_train.shape)"
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "501b6546-d6d0-4091-b917-dcfc6aab318e"
      },
      "source": [
        "Let's extract 5 random images from the train set and display them:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "65e1d9eb-7918-446d-845f-41481172aaac",
        "outputId": "d2015c5e-b900-4064-aa74-82254edc70cc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-00b4569da6a1>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "images = []\n",
        "plt.figure(figsize=(15,3))\n",
        "for i in range(5):\n",
        "    img = X_train[np.random.randint(0, 60000)].astype('float')\n",
        "    images.append(img)\n",
        "\n",
        "    plt.subplot(1,5,i+1)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')"
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbf8a980-a91e-4e0b-b6e2-785b55b5cd87"
      },
      "source": [
        "Next we use our **max_pool** and **avg_pool** tool to perform pooling on these five images and see how their results appear differently.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4c8c8b91-9022-4d6c-9c56-94d936b4f160"
      },
      "outputs": [],
      "source": [
        "max_pool = keras.Sequential([MaxPooling2D(pool_size = (2,2))])\n",
        "avg_pool = keras.Sequential([AveragePooling2D(pool_size = (2,2))])\n",
        "\n",
        "fig1, axs1 = plt.subplots(1, 5, figsize=(12,3), constrained_layout=True)\n",
        "fig2, axs2 = plt.subplots(1, 5, figsize=(12,3), constrained_layout=True)\n",
        "fig1.suptitle(\"Max pooling result\", fontsize=20)\n",
        "fig2.suptitle(\"Average pooling result\", fontsize=20)\n",
        "\n",
        "for img, ax1, ax2 in zip(images, axs1.flat, axs2.flat):\n",
        "    input_ = img.reshape(1, 28, 28, 1)\n",
        "    ax1.imshow(max_pool.predict(input_).squeeze(0).squeeze(2))\n",
        "    ax1.axis('off')\n",
        "    ax2.imshow(avg_pool.predict(input_).squeeze(0).squeeze(2))\n",
        "    ax2.axis('off')\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d90df42-3243-496c-abd2-10eaa332079c"
      },
      "source": [
        "As you can see, in max pooling the prominent features are highlighted more, whereas in average pooling it gives a smoother image retaining the complete features in the image.\n",
        "\n",
        "We cannot say that one particular pooling method is better than the other generally. The choice of pooling operation should be made based on the problem at hand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7694cd01-05ec-442d-87b2-e338a359992a"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "In this exercise, you will support building a CNN to classify images from the famous CIFAR10 dataset.\n",
        "\n",
        "The pre-defined **load_cifar10** function will return a preprocessed cifar10 dataset, where:\n",
        "\n",
        "1. The pixel values in **X_train** and **X_test** are normalized float numbers.\n",
        "2. The **y_train** and **y_test** are one-hot encoded into 10-element binary vectors with a 1 for the index of the class value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcebccd1-d51f-4bbb-a9ce-d8d388b8eae1"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "def load_cifar10():\n",
        "    (trainX, trainY), (testX, testY) = datasets.cifar10.load_data()\n",
        "\n",
        "    trainX = trainX.astype('float32') / 255\n",
        "    testX = testX.astype('float32') / 255\n",
        "\n",
        "    trainY = to_categorical(trainY)\n",
        "    testY = to_categorical(testY)\n",
        "\n",
        "    return trainX, trainY, testX, testY"
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41fb35ec-f0e1-403d-9e0c-45c8bdafe51b"
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = load_cifar10()\n",
        "\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f17ef772-5674-4945-8b1d-7eeca890178f"
      },
      "source": [
        "### Exercise 1 - Display some images\n",
        "\n",
        "Write the code to display the first 25 images from the train set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f6cc378-a133-421a-bb2d-5ae1df6d41e7"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "plt.figure(figsize=(10,10))\n",
        "# TO DO"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9590764b-8e1a-4b12-a9c1-4d62088f59b6"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i+1)\n",
        "    plt.imshow(X_train[i])\n",
        "    plt.title(class_names[np.where(y_train[i]==1)[0][0]])\n",
        "    plt.axis(\"off\")\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcdaffb-0353-4dd8-b217-6cf8189a83d9"
      },
      "source": [
        "### Exercise 2 - Set up a Convolution layer\n",
        "\n",
        "Create a Conv2D layer called **Conv** with\n",
        "- 32 $3\\times3$ kernels\n",
        "- `'he_uniform'` kernel initializer\n",
        "- Padding\n",
        "- ReLu activation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99b9ac88-fe7a-47a6-a971-78e5b223cccd"
      },
      "outputs": [],
      "source": [
        "# TO DO\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69bb3b8e-d97d-48d2-99c7-a985fbcb1003"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for a Solution</summary>\n",
        "    \n",
        "```python\n",
        "Conv = layers.Conv2D(filters=32,\n",
        "                     kernel_size=(3,3),\n",
        "                     kernel_initializer='he_uniform',\n",
        "                     padding='same',\n",
        "                     activation='relu')\n",
        "```\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27c4c35-9af8-4433-a94b-2aa3a8b2b7ef"
      },
      "source": [
        "### Exercise 3 - Set up a Max pooling layer\n",
        "\n",
        "Create a MaxPooling2D layer with pool_size equal 2, name the single layer **Max**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9552da-5b98-4275-be55-ce9a0ddf84af"
      },
      "outputs": [],
      "source": [
        "# TO DO\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97fa847c-9709-4575-9498-ac757104d983"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "Max = layers.MaxPooling2D(pool_size=(2,2))\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b80aecf5-ab41-46a4-b827-d4d6a24fc103"
      },
      "source": [
        "### Exercise 4 - Create a deeper CNN with blocks\n",
        "\n",
        "Now that you've practiced defining the two types of most commonly used layers in CNN. You can stack two **Conv** layers and one **Max** layer together as a block, and create a deeper CNN with three of those blocks!\n",
        "\n",
        "You could choose to double the number of kernels/channels in the **Conv2D** layers as you move from one block to another.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f22b51c7-f7ef-4dc6-bbec-23d6b35c8a3b"
      },
      "outputs": [],
      "source": [
        "# TO DO\n"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a61a28d9-c22d-4e66-8494-7ae21645fddb"
      },
      "source": [
        "<details>\n",
        "    <summary>Click here for Solution</summary>\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',\n",
        "                 input_shape=(32, 32, 3)))\n",
        "# Don't forget specifying input_shape in the 1st Conv2D layer of your CNN\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "```\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d32891d4-abf0-426a-a7d5-ec23a5026244"
      },
      "source": [
        "### Optional section\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "594e8ea2-9575-4dd5-8deb-419bd62acd7a"
      },
      "source": [
        "You've accomplished a lot so far! Now just run the following cells to finish up building your CNN with some Dense layers. Don't worry if you don't know what the code does, as you will learn about Flattening and Dense layers in the next lab.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1f48442-52e1-428f-8cde-2453b2767588"
      },
      "outputs": [],
      "source": [
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "model.summary()\n",
        "# compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd50ff4f-1c2f-4559-b9eb-1228bf9be757"
      },
      "source": [
        "# Congratulations! - You have completed the lab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35acf2f0-0bbb-4b1a-91a9-aac1ca7d9c78"
      },
      "source": [
        "## Authors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "916716bb-c13a-41a0-8ea6-124cc7eec67a"
      },
      "source": [
        "[Roxanne Li](https://www.linkedin.com/in/roxanne-li/) is a Data Scientist at IBM Skills Network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afcef9bf-0c8a-4b9f-a6be-f89366509a40"
      },
      "source": [
        "## Change Log\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "050e132d-5b7d-4c94-949b-312a16d98aa4"
      },
      "source": [
        "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
        "|-|-|-|-|\n",
        "|2020-07-17|0.1|Sam|Create Lab Template|\n",
        "|2022-06-10|0.1|Roxanne Li|Create Lab|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcb6f0d3-2a2f-4502-b197-e65d6b0e75dd"
      },
      "source": [
        "Copyright © 2022 IBM Corporation. All rights reserved.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python",
      "language": "python",
      "name": "conda-env-python-py"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "prev_pub_hash": "a0684e88235f7928d65f730559b38003ae5d6fe2e56b1a5a3884713a639fe504",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}